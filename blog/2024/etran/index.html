<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> [TL;DR] Energy-Based Transferability Estimation | Akshath Raghav R </title> <meta name="author" content="Akshath Raghav Ravikiran"> <meta name="description" content="TL;DR for the ICCV 2023 Paper"> <meta name="keywords" content="akshath, raghav, ravikiran, araviki, portfolio, resume, cv"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png?cb6145fc58382c78e56ae25fb78b7b23"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://akshathraghav.github.io/blog/2024/etran/"> <link defer rel="stylesheet" href="https://unpkg.com/leaflet@1.9.4/dist/leaflet.css" integrity="sha256-p4NxAoJBhIIN+hmNHrzRCf9tD/miZyoHS5obTRR9BMY=" crossorigin=""> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <u>a</u>kshath raghav <u>raviki</u>ran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/AkshathRaghav_Resume_Aug25.pdf">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">tl;dr </a> </li> <li class="nav-item "> <a class="nav-link" href="/photography/">photography </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">blog</a> <a class="dropdown-item " href="/gists/">gists</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">[TL;DR] Energy-Based Transferability Estimation</h1> <p class="post-meta"> August 06, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/tl-dr"> <i class="fa-solid fa-hashtag fa-sm"></i> tl;dr</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p><a href="https://github.com/AkshathRaghav/ptmrank/blob/main/ptmrank/metrics/ETran.py" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://gist.github.com/AkshathRaghav/996d18cecf3ca2076d1cbd640f02420a" rel="external nofollow noopener" target="_blank">Visualization Codde</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{gholami2023etranenergybasedtransferabilityestimation,
      title={ETran: Energy-Based Transferability Estimation}, 
      author={Mohsen Gholami and Mohammad Akbari and Xinglu Wang and Behnam Kamranian and Yong Zhang},
      year={2023},
      eprint={2308.02027},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.02027}, 
}
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/overview-480.webp 480w,/assets/img/tldr/etran/overview-800.webp 800w,/assets/img/tldr/etran/overview-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/overview.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <hr> <h2 id="what-is-it">What (is it)?</h2> <p>ETran was proposed on the hypothesis that evaluating whether target dataset features are in-distribution (IND) or out-of-distribution (OOD) for a given PTM would be enough for a good Transferrability Estimation (TE) metric. The core idea works based off the concept of statistical physics that correlates energy of a system to the likelihood of a sample being present in the distribution the source model (\(\phi\)) is trained on.</p> <p><strong>Cool Stuff</strong>:</p> <ul> <li>Makes explainable estimations about the knowledge relationship between source model and target dataset;</li> <li>Outlines a way to deal with object detection tasks using bounding boxes;</li> </ul> <p>The main part of the metric involves the idea behind Energy-Based Models (EBMs). EBMs are a class of models that use an energy function (may (or may not) be parameterized) to assign a scalar energy value to each possible configuration of the input data. It’s simple – anything that relates a low return value with good direction during training can be an energy function. For simpler models, this can be MSE or Cross-Entropy. If you’re getting more complex, certain layers or modules themselves should be made for this purpose.</p> <p>At all levels, the energy function is used to derive a probability distribution through the Gibbs distribution. The below equation shows that the probability of a particular outcome \(y\) given the input \(x\) decreases exponentially with increasing energy \(E(x,y)\). Just replace \(E(x,y)\) here with your energy function above and you’re set to run some visualizations on your PTM (\(\phi\)).</p> \[p(y|x) = \frac{e^{-E(x,y)}}{\int_{y'} e^{-E(x,y')}}\] <hr> <h2 id="why-do-we-need-it">Why (do we need it)?</h2> <ul> <li>Previous methods (think pre-2023 including SDFA and NLEEP) typically fail when the target dataset diverges significantly from the source dataset’s distribution. When the target dataset is OOD compared to the pre-trained model, the extracted features can become unreliable. (Note the word <em>unreliable</em> – it means “not robust”, but can be right sometimes.)</li> <li>The effectiveness of these methods is often compromised by the differences in features before and after fine-tuning, due to variations in source and target domains.</li> <li>ETran is also more efficient (think 2x and more) because it does not require extensive label information or complex optimization processes. The energy score is label- and optimization-free, making it a fast and easy-to-use metric for transferability estimation.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/time_complex-480.webp 480w,/assets/img/tldr/etran/time_complex-800.webp 800w,/assets/img/tldr/etran/time_complex-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/time_complex.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>(Please read the <a href="#when-does-it-fail">When (does it fail)?</a> section)</p> <hr> <h2 id="how-does-it-work">How (does it work)?</h2> <p>ETran’s transferability assessment includes three primary scores:</p> <ul> <li>Energy Score: Measures the likelihood of the target dataset being <em>compatible</em> with the pre-trained model.</li> <li>Classification Score: Uses LDA to project features into a higher D space, maximizing inter-class variance and minimizing intra-class variance.</li> <li>Regression Score: Uses SVD to evaluate the transferability for object detection tasks. <em>Pretty interesting</em>.</li> </ul> <h3 id="energy-score-s_en">Energy Score \(S_{en}\):</h3> <blockquote> <p>The <strong>energy function</strong> here is the model’s embedding layer itself. Thus, the outputted features act as the energy scores. The final score is the mean of all energy scores for the samples.</p> </blockquote> <p>Here is an optimized view of the process:</p> \[S_{en} = \frac{1}{|C|} \sum_{i \in C} \log \left( \sum_{j=1}^{d} e^{f_{ij}} \right)\] <p>where,</p> <ul> <li> <strong>\(f_{ij}\)</strong>: feature_vector[i][j] (\(i\) is the sample number and \(j\) is the vector dimension),</li> <li> <strong>\(d\)</strong>: n_dim,</li> <li> <strong>\(C\)</strong>: n_samples,</li> </ul> <p>Now, imagine we have Guassian clusters as our IND data and a Uniform distribution as our OOD data. This is how they’d look, assuming a sample size of 1000:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/initial-480.webp 480w,/assets/img/tldr/etran/initial-800.webp 800w,/assets/img/tldr/etran/initial-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/initial.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Instead of a whole model, and to ignore the pretext of training, we’ll use PCA for simplicity as an “embedding function”. Here’s how the “extracted” features look:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">extract_features</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="n">n_components</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/post-480.webp 480w,/assets/img/tldr/etran/post-800.webp 800w,/assets/img/tldr/etran/post-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/post.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>We’ll also take the Sum of Squares as the energy function. Here’s how we can find the energy and probability density.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_energy</span><span class="p">(</span><span class="n">features</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">features</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">calculate_probability_density</span><span class="p">(</span><span class="n">energy</span><span class="p">):</span>
    <span class="n">exp_neg_energy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">energy</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">exp_neg_energy</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_neg_energy</span><span class="p">)</span>
</code></pre></div></div> <p>Now, let’s see how these distributions look as a KDE plot:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/kde-480.webp 480w,/assets/img/tldr/etran/kde-800.webp 800w,/assets/img/tldr/etran/kde-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/kde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Note</strong>:</p> <ul> <li>When looking at the energy scores, the KDE plot show that IND samples have lower energy (peak towards the left) while OOD samples have a higher energy (peak towards right). This validates the initial hypothesis.</li> <li>When looking at the probability densities, the KDE plot shows that INR samples have higher densities (shifted towards right) while OOD samples have lower density (peak towards left). This confirms the method of using Gibbs.</li> </ul> <p>This method is employed practically for the following examples:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/examplekde-480.webp 480w,/assets/img/tldr/etran/examplekde-800.webp 800w,/assets/img/tldr/etran/examplekde-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/examplekde.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <i>You might notice that models with higher accuracies have peaks at lower energy. This is discussed below under the limitations of purely using the energy score.</i> </div> <hr> <h3 id="classification-score-s_cls">Classification Score (\(S_{cls}\)):</h3> <p>The Classification score projects the extracted features into a discriminative space, and tries to maximize the seperability between the classes. Then, by taking the average of the posterior probabilities of each class being predicted from this projection, it tries to guage how well the features (with minimized inter-class variance) align with the target dataset’s classes.</p> <blockquote> <p>Note that the <strong>energy function</strong> here is \(\mathbf{U}^T (\mathbf{f}_i^{(y_i)} - \mu_{y_i})\). The classification score normalizes the discriminant scores (mat-mul of the projected matrix against the normalized feature column) across all classes to compute a posterior probability for the true class.</p> </blockquote> <p>Here is an optimal view of the method:</p> \[S_{\text{cls}} = \frac{1}{N} \sum_{i=1}^{N} \frac{e^{\mathbf{U}^T (\mathbf{f}_i^{(y_i)} - \mu_{y_i})}}{\sum_{c=1}^{C} e^{\mathbf{U}^T (\mathbf{f}_i^{(c)} - \mu_c)}}\] <p>where,</p> <ul> <li> <strong>\(\mu_{y_i}\)</strong>: mean feature vector for the class (y_i).</li> <li> <strong>\(\mathbf{U}\)</strong>: projected feature matrix</li> <li> <strong>\(e^{\mathbf{U}^T (\mathbf{f}_i^{(y_i)} - \mu_{y_i})}\)</strong>: discriminant score for the true class (y_i).</li> <li> <strong>\(\frac{e^{\mathbf{U}^T (\mathbf{f}_i^{(y_i)} - \mu_{y_i})}}{\sum_{c=1}^{C} e^{\mathbf{U}^T (\mathbf{f}_i^{(c)} - \mu_c)}}\)</strong>: normalized posterior probability for the true class (y_i). <u> This fraction represents how well the \(i\)-th sample is classified into its true class compared to other classes. </u> </li> </ul> <p>First, consider a dataset of 1000 samples, with equal sets of Gaussian, Uniform and Exponentially sampled points. Each distribution type is assigned a class. This is how we ideally expect the data modelling to be, so that there is an ability to differentiate the features statistically.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gaussian_embeddings</span> <span class="o">=</span> <span class="n">npr</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="n">uniform_embeddings</span> <span class="o">=</span> <span class="n">npr</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
<span class="n">exponential_embeddings</span> <span class="o">=</span> <span class="n">npr</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/cls_initial-480.webp 480w,/assets/img/tldr/etran/cls_initial-800.webp 800w,/assets/img/tldr/etran/cls_initial-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/cls_initial.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="4xample image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <i>Remember, this is a naive example*</i> </div> <p>Now, take a look at how the probabilities get distributed when we treat the discriminant scores at the energy function:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/cls_kde-480.webp 480w,/assets/img/tldr/etran/cls_kde-800.webp 800w,/assets/img/tldr/etran/cls_kde-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/cls_kde.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Note</strong>:</p> <ul> <li>Class 0: A peak near a probability of 0.5 with a good spread towards lower probabilities. This indicates that the model is less confident in distinguishing Class 2, and there might be more misclassifications.</li> <li>Class 1: A sharp peak near a probability of 0.4, with a narrow spread. This indicates that most samples assigned to Class 1 have high probabilities, and the model is confident in these predictions.</li> <li>Class 2: A distinct spread with a local peak at 0.9. This would indicate that the model is able to confidently classify in only few of the cases. In the rest of them, it is unable to distinguish the class properly.</li> </ul> <p>From this, we can take a normalized average and use it as a score to estimate the overal confidence of the model. A model with <em>high \(S_{cls}\)</em> would have <strong>narrow spread, tall peaks towards the right</strong>.</p> <hr> <h3 id="regression-score-s_r">Regression Score (\(S_{r}\)):</h3> <p>The goal of this score is to understand how well the bounding boxes can be reconstructed using the features, assuming a linear relationship by SVD. The regression score is the MSE between the real and predicted \(j\) values for each bounding box \(k\). <u>Lower the score, the better the model will be.</u></p> \[S_{\text{reg}} = - \frac{1}{K \times 4} \sum_{k=1}^{K} \sum_{j=1}^{4} (b_{k}^{(j)} - \hat{b}_{k}^{(j)})^2\] <p>where,</p> <ul> <li> <strong>\(K\)</strong>: #bounding_boxes</li> <li> <strong>\(b_k^{(j)}\)</strong>: \(j\)-th coordinate of the \(k\)-th bounding box’s true label.</li> <li> <strong>\(\hat{b}_k^{(j)}\)</strong>: \(j\)-th coordinate of the \(k\)th bounding box’s predicted label,</li> </ul> <p>To visualize how this works, we’ll replicate the paper’s methodology, but ignoring the truncation to retrain only 80% of energy values. We will see why I did this in the <a href="#when-does-it-fail">When (does it fail)?</a> section.</p> <p>In my naive example, I’m using CLIP’s embedding function from <a href="https://huggingface.co/openai/clip-vit-base-patch32" rel="external nofollow noopener" target="_blank">this link</a> to generate features from this image. We’ll focus on a bounding box for the top-left square.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/plot-480.webp 480w,/assets/img/tldr/etran/plot-800.webp 800w,/assets/img/tldr/etran/plot-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">energy</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">S</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">S</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">S_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">S</span><span class="p">)</span>
<span class="n">features_pseudo_inv</span> <span class="o">=</span> <span class="n">Vh</span><span class="p">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">S_inv</span> <span class="o">@</span> <span class="n">U</span><span class="p">.</span><span class="n">T</span>
<span class="n">bboxes_approximated</span> <span class="o">=</span> <span class="n">features</span> <span class="o">@</span> <span class="n">features_pseudo_inv</span> <span class="o">@</span> <span class="n">targets</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>

<span class="n">regression_score</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">targets</span> <span class="o">-</span> <span class="n">bboxes_approximated</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">targets</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">4</span><span class="p">))[[</span> <span class="mf">69.99993</span>  <span class="mf">64.99993</span> <span class="mf">159.99983</span> <span class="mf">154.99985</span><span class="p">]]</span>
</code></pre></div></div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/bounding-480.webp 480w,/assets/img/tldr/etran/bounding-800.webp 800w,/assets/img/tldr/etran/bounding-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/bounding.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <i>Real: [[ 70 65 160 155 ]]</i> <i>Predicted: [[ 69.99993 64.99993 159.99983 154.99985]]</i> </div> <hr> <h2 id="when-does-it-fail">When (does it fail)?</h2> <p>Dealing purely with \(S_{en}\) is unreliable. This only interfaces between target \(y_{t}\) embeddings and the \(\phi\) embedding functions.</p> <ul> <li>Consider a PTM trained on a source dataset \(y_{s}\) whose features closely matches the target dataset (imagine two datasets with a lot of cat pictures). Now, if you get the energy score for this (\(\phi, y_{t}\)) pair, the \(E(x, y_{t})\) score will be low, and thus high likelihood.</li> <li>However, we’re completely <em>ignoring</em> the rest of the network and the classification labels. Different PTMs with similar embedding layers might be clustered together, regardless of their accuracy.</li> <li>Thus, use classification based scores like LogME, LEEP, PACTran and \(S_{cls}\) <strong>in addition to</strong> the \(S_{en}\) score. Reference the following table to note that \(S_{en}\) does not outperform the combined metrics.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/table-480.webp 480w,/assets/img/tldr/etran/table-800.webp 800w,/assets/img/tldr/etran/table-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/table.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>\(S_{cls}\) makes a number of assumptions, which might not hold good in the following cases:</p> <ul> <li>LDA assumes that the features within each class are normally distributed in the discriminative space: \(\mathbf{\bar{f}}^{(c)} \sim \mathcal{N}(\mathbf{U}^T \mu_c, \mathbf{I})\). The projections might not accurately reflect the seperability in this case.</li> <li>When the #dim » #samples_per_class, the variance matrices calculated will ill-conditioned. It will consequently not be positive semidefinite, and thus is unreliable. It will also lead to underflow errors.</li> <li>If the feature distributions of different classes <strong>significantly</strong> overlap, the LDA projection will not clearly separate these classes. This overlap can lead to ambiguous posterior probabilities, as the model will not be confident about class distinctions.</li> </ul> <p>\(S_{r}\) also makes a few assumptions:</p> <ul> <li>SVD assumes that the relationships between features and labels can be approximated lienarly. This might oversimplify complex non-linear relationships in real-world data.</li> <li>If we truncate 80% of singular values, it might lead to a loss of information and thus inability to reconstruct the boxes as needed. For example, consider the same example above, but with 80% truncation. On the right, we see no bounding box plotted. This is because the approximated values are \([0,0,0,0]\). If the remaining singular values don’t adequately represent the features needed to reconstruct the bounding boxes, the reconstruction could default to something uninformative, like the origin.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/tldr/etran/bounding_fail-480.webp 480w,/assets/img/tldr/etran/bounding_fail-800.webp 800w,/assets/img/tldr/etran/bounding_fail-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/tldr/etran/bounding_fail.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Akshath Raghav Ravikiran. Last updated: August 21, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://unpkg.com/leaflet@1.9.4/dist/leaflet.js" integrity="sha256-20nQCchB9co0qIjJZRGuk2/Z9VM+kNiyxNV1lvTlZBo=" crossorigin=""></script> <script>document.onreadystatechange=()=>{"complete"===document.readyState&&document.querySelectorAll("pre>code.language-geojson").forEach(e=>{const t=e.textContent,a=e.parentElement;a.classList.add("unloaded");let o=document.createElement("div");o.classList.add("map"),a.after(o);var n=L.map(o);L.tileLayer("https://tile.openstreetmap.org/{z}/{x}/{y}.png",{maxZoom:19,attribution:'&copy; <a href="http://www.openstreetmap.org/copyright">OpenStreetMap</a>'}).addTo(n);let d=L.geoJSON(JSON.parse(t)).addTo(n);n.fitBounds(d.getBounds())})};</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>