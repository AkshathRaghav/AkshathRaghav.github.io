<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> grammarflow | Akshath Raghav R </title> <meta name="author" content="Akshath Raghav Ravikiran"> <meta name="description" content="Powering Agent Chains by Constraining LLM Outputs 🪢"> <meta name="keywords" content="akshath, raghav, ravikiran, araviki, portfolio, resume, cv"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/icon.png?cb6145fc58382c78e56ae25fb78b7b23"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://akshathraghav.github.io/projects/grammarflow/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <u>a</u>kshath raghav <u>raviki</u>ran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">tl;dr </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/AkshathRaghavR_Resume.pdf">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">blog</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/gists/">gists</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/photography/">photography</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">grammarflow</h1> <p class="post-meta"> August 16, 2024 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2024   ·   <i class="fa-solid fa-tag fa-sm"></i> software   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>🚀 Supercharging Agent Chains with Constrained LLM outputs 🚀</p> <p>Find the package <a href="https://github.com/e-lab/SyntaxShaper/tree/main" rel="external nofollow noopener" target="_blank">here</a></p> <h2 id="overview">Overview</h2> <p>GrammarFlow abstracts the <strong>LLM constraining process for complex-response tasks</strong>. It helps you define your grammar rules using Pydantic and Typing in a pythonic way, and inherently embeds metadata from these dataclasses into the prompt. Parsing is enabled in JSON, TOML and XML formats, with custom parsers that avoid the issues faced by <code class="language-plaintext highlighter-rouge">json.loads</code> (..etc) while parsing direct outputs.</p> <p>Importantly, the package supports the generation of <strong>GNBF grammar</strong>, which integrates seamlessly with the <a href="https://github.com/ggerganov/llama.cpp/" rel="external nofollow noopener" target="_blank">llama.cpp</a> package. This integration allows for more intelligent sampling of logits, optimizing the response quality from models.</p> <p>The goal of this package was to overcome the issues faced when using LangChain’s output parsers with instruct language models locally. While GPT-4 produces consistent results in returning the correct formats, local models from families like Llama and Mistral would cause parsing errors in my testing chains when I need more than just a single string response. Recently, GrammarFlow was extended to cover more features to help anyone trying to work with LLMs for complex use-cases: multi-grammar generation, regex patterns, etc.</p> <blockquote> <p>Please reach out to <code class="language-plaintext highlighter-rouge">araviki[at]purdue[dot]edu</code> or open an issue on Github if you have any questions or inquiry related to GrammarFlow and its usage.</p> </blockquote> <h2 id="results">Results</h2> <p>GrammarFlow was tested against popular LLM datasets, with a focus on constraining model outputs. The goal was to ensure that the final parsed output matched both the <em>structure and data types</em> of the ground truth.</p> <p><a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/strategyqa/task.json" rel="external nofollow noopener" target="_blank">StrategyQA</a> - Simplest grammar (Nested Objects with str/int).</p> <p><a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/logic_grid_puzzle/" rel="external nofollow noopener" target="_blank">Logic Grid Puzzle</a> - Simple grammar (Nested Objects with lists), large prompts. &lt;= 500 words.</p> <p><a href="https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/reasoning_about_colored_objects/" rel="external nofollow noopener" target="_blank">ReasoningAboutColors</a> - Requires handling multiple fields in grammar.</p> <table class="table table-bordered table-sm"> <thead> <tr> <th style="text-align: center">Model Name</th> <th style="text-align: center">Parameters</th> <th style="text-align: center">Logic Grid Puzzle (n=200)</th> <th style="text-align: center">StrategyQA (n=200)</th> <th style="text-align: center">ReasoningAboutColors (n=200)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Mistral-7B</td> <td style="text-align: center">7B</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> </tr> <tr> <td style="text-align: center">CodeLlama2-13B</td> <td style="text-align: center">13B</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> </tr> <tr> <td style="text-align: center">Llama2-70B</td> <td style="text-align: center">70B</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> <td style="text-align: center">100.0%</td> </tr> </tbody> </table> <p><a href="https://github.com/Ber666/llm-reasoners/blob/main/examples/prontoqa/data/345hop_random_true.json" rel="external nofollow noopener" target="_blank">PrOntoQA</a> - Chain of Thought reasoning, with randomly-scattered supporting facts in prompt. Taken from <a href="https://github.com/Ber666/llm-reasoners/" rel="external nofollow noopener" target="_blank">llm-reasoners</a>. Tests the ability to place specific reasoning statements in the right place.</p> <p><a href="http://curtis.ml.cmu.edu/datasets/hotpot/hotpot_train_v1.1.json" rel="external nofollow noopener" target="_blank">HotPotQA</a> - Multi-hop questions, with strong supervision for supporting facts. Integrated within the first ReAct prompting paper’s <a href="https://github.com/ysymyth/ReAct" rel="external nofollow noopener" target="_blank">code</a>. Incremental steps, leading to large prompts; tests robustness.</p> <table class="table table-bordered table-sm"> <thead> <tr> <th style="text-align: center">Model Name</th> <th style="text-align: center">Parameters</th> <th style="text-align: center">PrOntoQA Parsing (n=200)</th> <th style="text-align: center">PrOntoQA Accuracy (n=200)</th> <th style="text-align: center">HotPotQA Parsing (n=200)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Mistral-7B</td> <td style="text-align: center">7B</td> <td style="text-align: center">99%</td> <td style="text-align: center">88.5%</td> <td style="text-align: center">99.0%</td> </tr> <tr> <td style="text-align: center">CodeLlama2-13B</td> <td style="text-align: center">13B</td> <td style="text-align: center">97.5%</td> <td style="text-align: center">55.5%</td> <td style="text-align: center">100.0%</td> </tr> <tr> <td style="text-align: center">Llama2-70B</td> <td style="text-align: center">70B</td> <td style="text-align: center">99.5%</td> <td style="text-align: center">81.9%</td> <td style="text-align: center">99.0%</td> </tr> </tbody> </table> <h2 id="features">Features</h2> <p>GrammarFlow is mainly meant to be an add-on to your existing LLM applications. It works on the input to and output from your <code class="language-plaintext highlighter-rouge">llm()</code> call, treating everything in between as a black box. It contains pre-made template prompts for local GGUF models like <a href="https://huggingface.co/TheBloke/Upstage-Llama-2-70B-instruct-v2-GGUF" rel="external nofollow noopener" target="_blank">Llama2 (70B, 13B, 7B)</a>, <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF" rel="external nofollow noopener" target="_blank">Mistral</a>, <a href="https://huggingface.co/TheBloke/Synthia-MoE-v3-Mixtral-8x7B-GGUF" rel="external nofollow noopener" target="_blank">Mixtral</a> and has template grammars for common tasks like Chain-of-Thought and Iterative Agents. Making these prompts and grammars are trivial and require minimal effort, as long as you know the format of what you’re building.</p> <ul class="task-list"> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked><strong>GBNF Support</strong>: Converts any Pydantic model to GNBF grammar for using with <a href="https://github.com/ggerganov/llama.cpp/" rel="external nofollow noopener" target="_blank">llama.cpp</a>’s token-based sampling. Enables adding regex patterns directly.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked><strong>Easy Integration</strong>: Integrates with any package or stack by just manipulating the prompt and decoding the result into a pythonic data abstractor. Treats everything in between as a <strong>black box</strong>.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked><strong>Handles Complex Grammars</strong>: Can handle typing objects (‘List’, ‘Dict’, etc.) and nested Pydantic logic with complex data-types.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked><strong>Experiments with different ‘formats’</strong>: Defines grammar rules in XML, JSON and TOML formats. JSON is the standard, while XML is best for nested parsing and TOML is best when you want to get multiple models parsed simulatenously. Each has it’s own usecase as described in the <a href="https://github.com/e-lab/SyntaxShaper/blob/main/samples/demo.ipynb" rel="external nofollow noopener" target="_blank">demo</a>.</li> <li class="task-list-item"> <input type="checkbox" class="task-list-item-checkbox" disabled checked><strong>Reduces hallucinations or garbage results during sampling</strong>: GBNF grammars allow for controlled whitespacing/identation and model ordering, while parsing logic allows for ignoring incorrect terminal symbols.</li> </ul> <h2 id="remarks">Remarks</h2> <p>Please keep in mind that this package is purely software driven and aims to make developers lives simpler. It can work across model families and parameter counts with great success in parsing.</p> <p>However, with an increase in complexity of the prompt, the accuracy and ‘performance’ of the model’s thinking capability will degrade. This is attributed to the context-window problem that a lot of researchers are working to improve. LLMs are autoregressive models which track previously seen tokens in order to iteratively predict the next one, and thus provide (a lot) of token probabilities in every generation. Different decoding startegies like <strong>nucleus sampling</strong> (used in GPT) and <strong>beam search</strong> are expensive and need to be used in combination with other methods to prune bad thinking patterns at generation time.</p> <p>In language models, a larger prompt provides more context, leading to a wider range of plausible continuations and increasing the uncertainty in the next token’s prediction. Mathematically, this manifests as a <strong>higher entropy in the distribution</strong> over possible next tokens, reflecting a greater number of likely follow-up sequences or “divergent trees” during decoding. Incorporating grammar-based constraining in language models forces the parsing of outputs to adhere to predefined syntactic rules, increasing the computational complexity and reducing flexibility in generation. This constriction <strong>narrows the search space of possible outputs</strong>, complicating the task of finding optimal sequences that satisfy both grammatical and contextual criteria.</p> <p>This is why people have come up with great workarounds like prompting strategies, prompt pruning, batch processing prompts (like in <a href="https://github.com/1rgs/jsonformer/blob/main/jsonformer/" rel="external nofollow noopener" target="_blank">JSONFormer</a> and <a href="https://github.com/varunshenoy/super-json-mode/blob/main/superjsonmode/" rel="external nofollow noopener" target="_blank">super-json-mode</a>), etc. Using those practices along with this library <strong>boosts the efficiency</strong> of whatever you’re building!</p> <div class="remark"> <div class="theorem-title">Remark (GrammarFlow Vs. Alternatives) </div> <div class="theorem-contents"> Batch-processing techniques entail generating simple strings in batches and subsequently formatting them into JSON structures manually. This approach, while straightforward, encounters significant limitations when the generated content requires internal consistency or interdependence among fields. For instance, take the generation of responses for a Chain of Thought (CoT) prompt. Traditional batch processing might yield a series of isolated responses, each reflecting distinct, possibly unrelated thought processes. When these responses need to be structured into a JSON format that adheres to a list, manual entry is not sufficient. This method lacks the capability to ensure that subsequent entries are contextually aligned with previous ones. This is where GrammarFlow steps in -- leveraging context-free grammars (CFGs) combined with carefully engineered prompts to guide the generation process. </div> </div> <h2 id="examples--samples">Examples (@ samples/)</h2> <ol> <li>For a general overview of what GrammarFlow can do, look at <a href="https://github.com/e-lab/SyntaxShaper/blob/main/samples/demo.ipynb" rel="external nofollow noopener" target="_blank">demo.ipynb</a>.</li> <li>For my modification to <a href="https://github.com/ysymyth/ReAct" rel="external nofollow noopener" target="_blank">ReAct’s</a> evaluation code on <a href="https://hotpotqa.github.io/" rel="external nofollow noopener" target="_blank">HotPotQA</a>, look at <a href="https://github.com/e-lab/SyntaxShaper/blob/main/samples/hotpotqa/hotpotqa_modified.ipynb" rel="external nofollow noopener" target="_blank">hotpotqa_modified</a>.</li> <li>I’ve also added an implementation of a <a href="https://github.com/e-lab/SyntaxShaper/blob/main/samples/bert_finetuning/annotator.ipynb" rel="external nofollow noopener" target="_blank">data annotator</a> for this <a href="https://www.datasciencecentral.com/how-to-fine-tune-bert-transformer-with-spacy-3/" rel="external nofollow noopener" target="_blank">BERT fine-tuning guide</a>.</li> </ol> <h2 id="quick-install">Quick Install</h2> <p><code class="language-plaintext highlighter-rouge">pip install grammarflow</code></p> <h2 id="code-usage">Code Usage</h2> <p>Map out what your agent chain is doing. Understand what it’s goals are and what data needs to be carried forward from one step to the next. For example, consider the <a href="https://react-lm.github.io/" rel="external nofollow noopener" target="_blank">ReAct prompting framework</a>. In every call, we want to pass in the Action and subsequent Observation to the next call.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">grammarflow</span> <span class="kn">import</span> <span class="o">*</span> 
<span class="kn">from</span> <span class="n">grammarflow.prompt.template</span> <span class="kn">import</span> <span class="n">Agent</span> 
<span class="kn">from</span> <span class="n">grammarflow.grammars.template</span> <span class="kn">import</span> <span class="n">AgentStep</span>
<span class="kn">from</span> <span class="n">grammarflow.tools.LLM</span> <span class="kn">import</span> <span class="n">LocalLlama</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">LocalLlama</span><span class="p">()</span> 
<span class="n">prompt</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">()</span> 
<span class="c1"># prompt.placeholders lists out what you can pass into the prompt. 
</span>
<span class="n">system_context</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Your goal is to think and plan out how to solve questions using agent tools provided to you. Think about all aspects of your thought process.</span><span class="sh">"""</span>
<span class="n">user_message</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Who is Vladmir Putin?</span><span class="sh">"""</span>

<span class="k">with</span> <span class="nc">Constrain</span><span class="p">(</span><span class="sh">'</span><span class="s">xml</span><span class="sh">'</span><span class="p">)</span> <span class="k">as</span> <span class="n">manager</span><span class="p">:</span>
    <span class="c1"># Makes the changes to the prompt
</span>    <span class="n">prompt</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
        <span class="n">prompt</span><span class="p">,</span>
        <span class="n">placeholders</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">prompt</span><span class="sh">'</span><span class="p">:</span> <span class="n">user_message</span><span class="p">,</span> <span class="sh">'</span><span class="s">instructions</span><span class="sh">'</span><span class="p">:</span> <span class="n">system_context</span><span class="p">},</span>
        <span class="n">grammars</span><span class="o">=</span><span class="p">[{</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">:</span> <span class="n">AgentStep</span><span class="p">}]</span>
    <span class="p">)</span>

    <span class="n">llm_response</span> <span class="o">=</span> <span class="nf">llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

    <span class="c1"># Parse the response into a custom dataclass for holding values
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">manager</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">llm_response</span><span class="p">)</span>

<span class="n">observation</span> <span class="o">=</span> <span class="nc">PerformSomeAction</span><span class="p">(</span>
  <span class="n">action</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">AgentStep</span><span class="p">.</span><span class="n">action</span><span class="p">,</span> 
  <span class="n">action_input</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">AgentStep</span><span class="p">.</span><span class="n">action_input</span>
<span class="p">)</span> 
</code></pre></div></div> <h2 id="gnbf-grammar">GNBF Grammar</h2> <div class="remark"> <div class="theorem-title">Remark (what is it?) </div> <div class="theorem-contents"> GBNF (GGML BNF) is a format for defining formal grammars to constrain model outputs in llama.cpp. For example, you can use it to force the model to generate valid JSON, or speak only in emojis. </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define your model 
</span><span class="k">class</span> <span class="nc">TeamMember</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">role</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">class</span> <span class="nc">TaskUpdate</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">update_time</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">comment</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">status</span><span class="p">:</span> <span class="nb">bool</span>

<span class="k">class</span> <span class="nc">Task</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">title</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">assigned_to</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TeamMember</span><span class="p">]</span>
    <span class="n">due_date</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">updates</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TaskUpdate</span><span class="p">]</span>

<span class="k">class</span> <span class="nc">Project</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">project_url</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">team_members</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">TeamMember</span><span class="p">]</span>
    <span class="n">grammars</span><span class="p">:</span> <span class="n">Task</span>

<span class="c1"># Convert to grammar
</span><span class="kn">from</span> <span class="n">grammarflow</span> <span class="kn">import</span> <span class="n">GNBF</span>

<span class="n">grammar</span> <span class="o">=</span> <span class="nc">GNBF</span><span class="p">(</span><span class="n">Project</span><span class="p">).</span><span class="nf">generate_grammar</span><span class="p">(</span><span class="n">format_</span><span class="o">=</span><span class="sh">'</span><span class="s">json</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Verify with LlamaGrammar
</span><span class="n">GNBF</span><span class="p">.</span><span class="nf">verify_grammar</span><span class="p">(</span><span class="n">grammar</span><span class="p">,</span> <span class="p">)</span>
</code></pre></div></div> <p>This is what gets returned, a version of BNF grammar rules:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>root ::= project ws
project ::= "{" ws "\"name\":" ws string "," ws "\"description\":" ws string "," ws "\"project-url\":" ws string "," ws "\"team-members\":" ws teammember "," ws "\"grammars\":" ws grammars "}" ws
ws ::= [ \t\n]*
string ::=  "\"" (
            [^"\\] |
            "\\" (["\\/bfnrt] | "u" [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f] [0-9a-fa-f])
            )* "\""
teammember ::= "{" ws "\"name\":" ws string "," ws "\"role\":" ws string "}" ws
number ::= ("-"? ([0-9] | [1-9] [0-9]*)) ("." [0-9]|)? ([ee] [-|]? [0-9]|)?
taskupdate ::= "{" ws "\"update-time\":" ws number "," ws "\"comment\":" ws string "," ws "\"status\":" ws status "}" ws
array ::= "[" ws (
                due-date-value
                ("," ws due-date-value)*
            )? "]" ws
due-date-value ::= string
task ::= "{" ws "\"title\":" ws string "," ws "\"description\":" ws string "," ws "\"assigned-to\":" ws teammember "," ws "\"due-date\":" ws array "," ws "\"updates\":" ws taskupdate "}" ws
</code></pre></div></div> <p>You can use this grammar to pass into <a href="https://github.com/ggerganov/llama.cpp/" rel="external nofollow noopener" target="_blank">llama.cpp</a> through a barebones LLM class that is provided.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">llm</span> <span class="o">=</span> <span class="nc">LocalLlama</span><span class="p">()</span> 
<span class="n">response</span> <span class="o">=</span> <span class="nf">llm</span><span class="p">(</span><span class="n">manager</span><span class="p">.</span><span class="n">prompt</span><span class="p">,</span> <span class="n">grammar</span><span class="o">=</span><span class="n">manager</span><span class="p">.</span><span class="nf">get_grammar</span><span class="p">(</span><span class="n">CoT</span><span class="p">),</span> <span class="n">stop_at</span><span class="o">=</span><span class="n">manager</span><span class="p">.</span><span class="n">stop_at</span><span class="p">)</span>
</code></pre></div></div> <h2 id="citation">Citation</h2> <p>We appreciate it if you would please cite this repo if you found the library useful for your work:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@software{GrammarFlow,
  author = {Ravikiran, Akshath Raghav and Culurciello, Eugenio},
  title = {GrammarFlow: Powering Agent Chains by Constraining LLM Outputs},
  year = {2024},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/e-lab/GrammarFlow}}, 
  version = {0.1.1}
}
</code></pre></div></div> </div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Akshath Raghav Ravikiran. Last updated: August 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>