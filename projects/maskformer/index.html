<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> maskformer | Akshath Raghav R </title> <meta name="author" content="Akshath Raghav Ravikiran"> <meta name="description" content="Re-engineering MaskFormer for Google's TF Model Garden (Funded) ðŸš€"> <meta name="keywords" content="akshath, raghav, ravikiran, araviki, portfolio, resume, cv"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="/assets/img/icon.png?cb6145fc58382c78e56ae25fb78b7b23"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://akshathraghav.github.io/projects/maskformer/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <u>a</u>kshath raghav <u>raviki</u>ran </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/summaries/">tl;dr </a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/AkshathRaghavR_Resume.pdf">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/gists/">gists</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">maskformer</h1> <p class="post-description">Re-engineering MaskFormer for Google's TF Model Garden (Funded) ðŸš€</p> </header> <article> <p>This page is the culmination of the work I did at <a href="https://github.com/PurdueDualityLab/tf-maskformer" rel="external nofollow noopener" target="_blank">Duality Lab</a> (Fall â€˜23 + Spring â€˜24) as a research assistant under <a href="https://www.linkedin.com/in/vishalsp/" rel="external nofollow noopener" target="_blank">Vishal S P</a> and <a href="https://wenxin-jiang.github.io/cv/" rel="external nofollow noopener" target="_blank">Wenxin Jiang</a>.</p> <div class="remark"> <div class="theorem-title">Remark (What &amp; Why?) </div> <div class="theorem-contents"> Re-engineered the state-of-the-art MaskFormer computer-vision model to publish into the TensorFlow Model Garden; Developed an implementation which ran efficiently on GCP TPUs out-of-box. </div> </div> <ul> <li>Wrote the evaluation module, including the implementation of panoptic inference metrics to work with tf2â€™s inbuilt task workflow. Fixed and integrated code for multi-scale (auxillary) losses to address inability of loss convergence.</li> <li>Conducted experiments on GPUs &amp; TPUs to ensure layer precision across the meta-architecture and implemented functions to ensure data consistency through the dataloader. Fixed run-time issues in main task file related to running across different computes.</li> <li>Responsible for the PR to <a href="https://github.com/tensorflow/models" rel="external nofollow noopener" target="_blank">TF Model Garden</a>, from re-writing modules (across layers) for guidelines adherence to creating/annotating results for the report. Wrote unit tests for all modules used in the architecture for tensor shape validation and expected results. Wrote <a href="https://gist.github.com/AkshathRaghav/9f81ea6f997a2972732fb3d955b5b444" rel="external nofollow noopener" target="_blank">differential testing module</a> for comparison of modules between PyTorch and TensorFlow.</li> </ul> <p>Find our code <a href="https://github.com/PurdueDualityLab/tf-maskformer/tree/PR_Draft/models/official/projects/maskformer" rel="external nofollow noopener" target="_blank">here</a>. Find the paper <a href="https://arxiv.org/pdf/2404.18801" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Below, Iâ€™ve summarized some portions from the paper, and included some code to get started with the model. A huge thank you to <strong>Vishal</strong> for his hard-work and patience!</p> <hr> <h1 id="description">Description</h1> <p>MaskFormer, a universal architecture based on MaskFormer meta-architecture that achieves SOTA on panoptic, instance and semantic segmentation across four popular datasets (ADE20K, Cityscapes, COCO, Mapillary Vistas).</p> <p>MaskFormer transforms any per-pixel classification model into a mask classification method. It utilizes a Transformer decoder to compute a set of pairs, each comprising a class prediction and a mask embedding vector. The binary mask prediction is obtained via a dot product between the mask embedding vector and per-pixel embedding from a fully-convolutional network. This model addresses both semantic and instance-level segmentation tasks without requiring changes to the model, losses, or training procedure. For both semantic and panoptic segmentation tasks, MaskFormer is supervised using the same per-pixel binary mask loss and a single classification loss per mask. A straightforward inference strategy is designed to convert MaskFormer outputs into a task-dependent prediction format.</p> <p align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/maskformer_real-480.webp 480w,/assets/img/maskformer/maskformer_real-800.webp 800w,/assets/img/maskformer/maskformer_real-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/maskformer_real.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </p> <h1 id="results">Results</h1> <p>Here are our results:</p> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/train_results-480.webp 480w,/assets/img/maskformer/train_results-800.webp 800w,/assets/img/maskformer/train_results-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/train_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Annotated results of our results on the training split of the COCO TFRecords </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/val_results-480.webp 480w,/assets/img/maskformer/val_results-800.webp 800w,/assets/img/maskformer/val_results-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/val_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Annotated results of our results on the eval split of the COCO TFRecords </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/losses-480.webp 480w,/assets/img/maskformer/losses-800.webp 800w,/assets/img/maskformer/losses-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/losses.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Comparison of our training losses vs PyTorch's logs. From left to right: 'cls_loss', 'dice_loss', 'focal_loss/mask_loss', 'total_loss' </div> </div> <h1 id="architecture">Architecture</h1> <h3 id="dataloader">DataLoader</h3> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/maskformer_data-480.webp 480w,/assets/img/maskformer/maskformer_data-800.webp 800w,/assets/img/maskformer/maskformer_data-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/maskformer_data.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> PreProcessing Outline </div> </div> <p>The process begins with the raw image and annotation files (JSON format), which are serialized into TFRecord format, a TensorFlow-specific binary storage format. Subsequently, these TFRecords are deserialized, and the data is passed through a series of preprocessing steps including normalization, augmentation, and padding to ensure uniformity of the input data. The binary mask generation and the label mapping are integral to preparing segmentation tasks. The output is then converted to a format compatible with the models data loader, which feeds the processed data into the training pipeline. (zoom in for a better viewing experience)</p> <h3 id="compressed-overview">Compressed Overview</h3> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/maskformer(3)-480.webp 480w,/assets/img/maskformer/maskformer(3)-800.webp 800w,/assets/img/maskformer/maskformer(3)-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/maskformer(3).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>After the dataloader step, the model continues with an input image (A), which is processed by a ResNet-50 backbone to extract feature maps. Multi-scale features (B) are generated from the backbone and then decoded by a Pixel Decoder (C) to create refined feature maps. These are fed into a Transformer Decoder (D), along with a set of learned queries that interact with the feature maps to generate mask embeddings (F). An MLP head (E) processes the mask embedding to produce the final segmentation output, which consists of binary masks for each query, along with their corresponding output labels, indicating the presence of specific objects or regions within the input image.</p> <h3 id="complete-implementation">Complete Implementation</h3> <div align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskformer/transformer_pixel_decoder(1)-480.webp 480w,/assets/img/maskformer/transformer_pixel_decoder(1)-800.webp 800w,/assets/img/maskformer/transformer_pixel_decoder(1)-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/maskformer/transformer_pixel_decoder(1).png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Annotated results of our results on the training split of the COCO TFRecords </div> </div> <p>The architecture begins with the backbone features extracted from the input image. These features are then projected and enriched with positional embeddings before being passed through multiple layers of a Transformer encoder. The resulting encoded features are processed by a Transformer Pixel Decoder, which combines them with mask features to enhance localization capability. Simultaneously, query embeddings are combined with positional embeddings and processed by the Transformer Decoder to generate predictions. The decoded features are then passed through an MLP head, which classifies each pixel, resulting in the final output comprising predicted binary masks and their associated labels.</p> <ul> <li> <p><strong>Backbone Network A</strong> : The original work explored a diverse array of backbone networks for fea- ture extraction, encompassing both convolutional architectures, such as ResNet-50 He et al. (2016), and transformer-based models like the Swin Transformer Liu et al. (2021). The ResNet50 imple- mentation is readily available in TFMG under official &gt; vision &gt; modeling &gt; backbones and the pre-trained checkpoint for ResNet are made available at TF-Vision Model Garden found under official &gt; vision.</p> </li> <li> <p><strong>Multiscale Features B</strong> : In our modelâ€™s architecture, a key component is the utilization of mul- tiscale features, as indicated by the â€˜Bâ€™ in Figure 5. This design choice allows for flexibility in feature extraction from the backbone network, specifically ResNet. Our implementation supports both the extraction of deep, high-level features from the final layer of ResNet and the rich, intermediate features available from earlier layers. An important consideration when employing the multiscale features is the increased demand for memory during the model training phase. The inclusion of data from multiple layers escalates the volume of information processed, which can strain available mem- ory resources. In our implementation, the use of multi-scale features is controlled by the parameter deep_supervision</p> </li> <li> <p><strong>Transformer Pixel Decoder C</strong> : A comprehensive depiction of the transformer pixel decoderâ€™s architecture is illustrated in Figure 6. At its core, the transformer pixel decoder is an assembly of two primary components: the transformer encoder and a series of convolutional and normalization layers. We have incorporated the transformer encoder layers into our architecture by utilizing the existing implementation available in the TensorFlow Model Garden, specifically from the directory official &gt; nlp &gt; layers. To maintain consistency and interoperability with models developed in other frameworks, particularly PyTorch, we have undertaken a process to ensure that our adopted transformer encoder layers exhibit functional parity with their PyTorch counterparts.</p> </li> <li> <p><strong>Transformer Decoder D</strong> : Similar to our approach with the Pixel Decoder, we have adopted the Transformer Decoder component from the pre-existing implementations provided in the Ten- sorFlow Model Garden, specifically sourced from the directory official &gt; nlp &gt; layers. While integrating the Transformer Decoder from the TensorFlow Model Garden, we ensure that it seam- lessly interfaces with other components of our model, such as the transformer pixel decoder and the backbone network. This involves aligning input and output dimensions, ensuring compatible data types, and configuring the decoder appropriately to match our specific requirements. We validate and test the functionality of the integrated transformer decoder and the transformer pixel decoder within our architecture.</p> </li> <li> <p><strong>MLP Projection E</strong> : The MLP (Multi-Layer Perceptron) projection layers, in conjunction with the linear classifier, are used to obtain the predicted binary masks and corresponding labels. The Transformer Decoder processes the Transformer Encoder Features to extract high-dimensional fea- tures, Transformer Features, that encapsulate both spatial and contextual information are used by the linear classifier to predict the label of each binary mask. Simultaneously, Mask Features that are obtained from the Transformer Pixel Decoder are fed into the MLP projection layers to obtain the predicted binary masks. We implement the Head of MaskFormer using TensorFlow API.</p> </li> </ul> <h1 id="getting-started">Getting Started</h1> <hr> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/PurdueDualityLab/tf-maskformer.git
cd models/official/projects/maskformer
</code></pre></div></div> <h2 id="environment-creation">Environment Creation</h2> <p>Create and activate a new conda environment to isolate the project dependencies:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create -n tfmaskformer
conda activate tfmaskformer
pip install -r /models/official/requirements.txt
pip install tensorflow-text-nightly
</code></pre></div></div> <h2 id="coco-dataset-to-tfrecord-conversion-guide">COCO Dataset to TFRecord Conversion Guide</h2> <p>Below are detailed instructions on how to convert the COCO dataset annotations and images into TensorFlowâ€™s TFRecord format using the <code class="language-plaintext highlighter-rouge">create_coco_tf_record.py</code> script.</p> <h3 id="overview">Overview</h3> <p>The <code class="language-plaintext highlighter-rouge">create_coco_tf_record.py</code> script processes raw COCO dataset files, including images and their annotations (object annotations, panoptic annotations, and captions), and encodes them into TFRecord files. Each image and its corresponding annotations are encapsulated in a <code class="language-plaintext highlighter-rouge">tf.Example</code> protobuf message, serialized, and written to a TFRecord file.</p> <p>To use this script, ensure you have the following COCO dataset files:</p> <ul> <li> <strong>Images Directory</strong>: For example, <code class="language-plaintext highlighter-rouge">train2017/</code> </li> <li> <strong>Annotations Files</strong>: Such as <code class="language-plaintext highlighter-rouge">instances_train2017.json</code>, <code class="language-plaintext highlighter-rouge">captions_train2017.json</code>, <code class="language-plaintext highlighter-rouge">panoptic_train2017.json</code> </li> <li> <strong>Panoptic Masks Directory</strong> (if using panoptic segmentation): Typically named similar to <code class="language-plaintext highlighter-rouge">train2017/</code> </li> </ul> <h3 id="running-the-script">Running the Script</h3> <p>Use the following command template to run the conversion script. Make sure to adjust the paths and filenames to match your datasetâ€™s location and your specific requirements:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python create_coco_tf_record.py <span class="se">\</span>
    <span class="nt">--image_dir</span><span class="o">=</span><span class="s2">"/path/to/coco/images/train2017"</span> <span class="se">\</span>
    <span class="nt">--object_annotations_file</span><span class="o">=</span><span class="s2">"/path/to/coco/annotations/instances_train2017.json"</span> <span class="se">\</span>
    <span class="nt">--caption_annotations_file</span><span class="o">=</span><span class="s2">"/path/to/coco/annotations/captions_train2017.json"</span> <span class="se">\</span>
    <span class="nt">--panoptic_annotations_file</span><span class="o">=</span><span class="s2">"/path/to/coco/annotations/panoptic_train2017.json"</span> <span class="se">\</span>
    <span class="nt">--panoptic_masks_dir</span><span class="o">=</span><span class="s2">"/path/to/coco/panoptic_masks/train2017"</span> <span class="se">\</span>
    <span class="nt">--include_masks</span><span class="o">=</span>True <span class="se">\</span>
    <span class="nt">--include_panoptic_masks</span><span class="o">=</span>True <span class="se">\</span>
    <span class="nt">--output_file_prefix</span><span class="o">=</span><span class="s2">"/path/to/output/tfrecords/train"</span> <span class="se">\</span>
    <span class="nt">--num_shards</span><span class="o">=</span>100
</code></pre></div></div> <p>Replace <code class="language-plaintext highlighter-rouge">/path/to/</code> with the actual paths where your COCO dataset images and annotations are stored. The <code class="language-plaintext highlighter-rouge">output_file_prefix</code> flag specifies the output directory and filename prefix for the generated TFRecord files, while the <code class="language-plaintext highlighter-rouge">num_shards</code> flag indicates how many shard files to create for the dataset.</p> <h3 id="important-options">Important Options</h3> <ul> <li> <strong>Include Masks</strong>: Set <code class="language-plaintext highlighter-rouge">--include_masks=True</code> to include instance segmentation masks in the TFRecords.</li> <li> <strong>Include Panoptic Masks</strong>: Use <code class="language-plaintext highlighter-rouge">--include_panoptic_masks=True</code> for panoptic segmentation tasks, which require both instance and semantic segmentation data.</li> <li> <strong>Sharding</strong>: Splitting the dataset into multiple shards (<code class="language-plaintext highlighter-rouge">--num_shards</code>) can significantly improve data loading efficiency during model training.</li> <li> <strong>Panoptic Annotations</strong>: For panoptic segmentation, both <code class="language-plaintext highlighter-rouge">--panoptic_annotations_file</code> and <code class="language-plaintext highlighter-rouge">--panoptic_masks_dir</code> must be specified.</li> </ul> <h2 id="example-scripts">Example Scripts</h2> <p>After completing the above steps, you can get started with training your model! Within the scripts folder you can find the training and evaluation scripts for different compute platforms (CPU/GPU/TPU).<br> <em>Remember to load the correct modules as required by each compute platform!</em></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scripts/
  eval_cpu.sh
  eval_gpu.sh
  eval_tpu.sh
  train_cpu.sh
  train_gpu.sh
  train_tpu.sh
</code></pre></div></div> <h2 id="recommended-working-with-tpus">(Recommended) Working with TPUs</h2> <h3 id="environment-variables">Environment Variables</h3> <p>Manually set the environment variables as follows:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export PYTHONPATH=$PYTHONPATH:~/models
export RESNET_CKPT={}
export MODEL_DIR={} # filepath to store logs
export TRAIN_BATCH_SIZE={}
export EVAL_BATCH_SIZE={}

export TPU_NAME={}
export TPU_SOFTWARE={}
export TPU_PROJECT={}
export TPU_ZONE={}
export TFRECORDS_DIR={} # .tfrecord datafolder

export ON_TPU=True 

export BASE_LR=0.0001
export IMG_SIZE=640
export NO_OBJ_CLS_WEIGHT=0.001

export DEEP_SUPERVISION=False 
</code></pre></div></div> <h3 id="training">Training</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export OVERRIDES="runtime.distribution_strategy=tpu,runtime.mixed_precision_dtype=float32,\
task.train_data.global_batch_size=$TRAIN_BATCH_SIZE,\
task.model.which_pixel_decoder=transformer_fpn,\
task.init_checkpoint=$RESNET_CKPT"
python3 models/official/projects/maskformer/train.py \
  --experiment maskformer_coco_panoptic \
  --mode train \
  --model_dir $MODEL_DIR \
  --tpu $TPU_NAME \
  --params_override $OVERRIDES
</code></pre></div></div> <h3 id="evaluation">Evaluation</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>export OVERRIDES="runtime.distribution_strategy=tpu,runtime.mixed_precision_dtype=float32,\
task.validation_data.global_batch_size=$EVAL_BATCH_SIZE,task.model.which_pixel_decoder=transformer_fpn,\
task.init_checkpoint_modules=all,\
task.init_checkpoint=$MASKFORMER_CKPT"
python3 train.py \
  --experiment maskformer_coco_panoptic \
  --mode eval \
  --model_dir $MODEL_DIR \
  --tpu $TPU_NAME \
  --params_override=$OVERRIDES
</code></pre></div></div> <h2 id="citation">Citation</h2> <p>If you find our implementation useful cite our work.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@misc{purohit2024partial,
      title={A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden}, 
      author={Vishal Purohit and Wenxin Jiang and Akshath R. Ravikiran and James C. Davis},
      year={2024},
      eprint={2404.18801},
      archivePrefix={arXiv},
}
</code></pre></div></div> </article> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Akshath Raghav Ravikiran. Last updated: June 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>