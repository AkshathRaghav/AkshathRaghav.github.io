---
layout: post
title: "ETran - Energy-Based Transferability Estimation"
date: 2024-08-06 17:57:00
description: TL;DR for the ICCV 2023 Paper
tags: research 
categories: 
map: true
---

[Paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Gholami_ETran_Energy-Based_Transferability_Estimation_ICCV_2023_paper.pdf)
[Code](https://github.com/AkshathRaghav/ptmrank/blob/main/ptmrank/metrics/ETran.py)

{% include figure.liquid loading="eager" path="assets/img/tldr/etran/overview.jpeg" title="example image" class="img-fluid rounded z-depth-1" %}

## What (is it)?

ETran was proposed on the hypothesis that evaluating whether target dataset features are in-distribution (IND) or out-of-distribution (OOD) for a given PTM would be enough for a good Transferrability Estimation (TE) metric. The core idea works based off the concept of statistical physics that correlates energy of a system to the likelihood of a sample being present in the distribution the source model ($$\phi$$) is trained on. 

**Cool Stuff**: 
- Outlines a way to deal with object detection tasks using bounding boxes;
- Ignores the complexities of handling target labels; 
- Makes explainable estimations about the knowledge relationship between source model and target dataset; 

The main part of the metric involves the idea behind Energy-Based Models (EBMs).  EBMs are a class of models that use an energy function (may (or may not) be parameterized) to assign a scalar energy value to each possible configuration of the input data. It's simple -- anything that relates a low return value with good direction during training can be an energy function. For simpler models, this can be MSE or Cross-Entropy. If you're getting more complex, certain layers or modules themselves should be made for this purpose.

At all levels, the energy function is used to derive a probability distribution through the Gibbs distribution. The below equation shows that the probability of a particular outcome $$y$$ given the input $$x$$ decreases exponentially with increasing energy $$E(x,y)$$. Just replace $$E(x,y)$$ here with your energy function above and you're set to run some visualizations on your PTM ($$\phi$$).

$$ p(y|x) = \frac{e^{-E(x,y)}}{\int_{y'} e^{-E(x,y')}} $$ 


## Why (do we need it)?

* Previous methods (think pre-2023 including SDFA and NLEEP) typically fail when the target dataset diverges significantly from the source dataset's distribution. When the target dataset is OOD compared to the pre-trained model, the extracted features can become unreliable. (Note the word *unreliable* -- it means "not robust", but can be right sometimes.)
* The effectiveness of these methods is often compromised by the differences in features before and after fine-tuning, due to variations in source and target domains.
* ETran is also more efficient (think 2x and more) because it does not require extensive label information or complex optimization processes. The energy score is label- and optimization-free, making it a fast and easy-to-use metric for transferability estimation. 

{% include figure.liquid loading="eager" path="assets/img/tldr/etran/time_complex.jpeg" title="example image" class="img-fluid rounded z-depth-1" %}


(Please read the [When (does it fail)?](#when-does-it-fail) section)

## How (does it work)?

ETranâ€™s transferability assessment includes three primary scores:

* Energy Score: Measures the likelihood of the target dataset being *compatible* with the pre-trained model.
* Classification Score: Uses Linear Discriminant Analysis (LDA) to project features into a higher D space, maximizing inter-class variance and minimizing intra-class variance.
* Regression Score: Uses Singular Value Decomposition (SVD) to evaluate the transferability for object detection tasks. *Pretty interesting*.

### Energy Score $$S_{en}$$: 

Here is an optimized view of the process: 

$$ S_{en} = \frac{1}{|C|} \sum_{i \in C} \log \left( \sum_{j=1}^{d} e^{f_{ij}} \right) $$

where,  
* **$$f_{ij}$$**: feature_vector[i][j] ($$i$$ is the sample number and $$j$$ is the vector dimension),
* **$$d$$**: n_dim,
* **$$C$$**: n_samples,


Now, imagine we have Guassian clusters as our IND data and a Uniform distribution as our OOD data. This is how they'd look, assuming a sample size of 1000: 

{% include figure.html
    path="/assets/img/tldr/etran/initial.png"
    class="z-depth-1"
%}

Instead of a whole model, and to ignore the pretext of training, we'll use PCA for simplicity as an "embedding function". Here's how the "extracted" features look: 

{% include figure.html
    path="/assets/img/tldr/etran/post.png"
    class="z-depth-1"
%}

We'll also take the Sum of Squares as the energy function. Here's how we can find the energy and probability density.  

```
def extract_features(data, n_components=3):
    pca = PCA(n_components=n_components)
    return pca.fit_transform(data)

def calculate_energy(features):
    return np.sum(features**2, axis=1)

def calculate_probability_density(energy):
    exp_neg_energy = np.exp(-energy)
    return exp_neg_energy / np.sum(exp_neg_energy)
```

Now, let's see how these distributions look as a KDE plot: 

{% include figure.html
    path="/assets/img/tldr/etran/kde.png"
    class="z-depth-1"
%}

**Note**: 
* When looking at the energy scores, the KDE plot show that IND samples have lower energy (peak towards the left) while OOD samples have a higher energy (peak towards right). This validates the initial hypothesis.
* When looking at the probability densities, the KDE plot sh sthat INR samples have higher densities (shifted towards right) while OOD samples have lower density (peak towards left). This confirms the method of using Gibbs. 

This method is employed practically for the following examples: 

{% include figure.liquid loading="eager" path="assets/img/tldr/etran/kde.jpeg" title="example image" class="img-fluid rounded z-depth-1" %}


## When (does it fail)?

* Dealing purely with $$S_{en}$$ is **unreliable**. This purely interfaces between target embeddings and the source model's embedding functions. Consider a classification task for a PTM with an embedding function which was trained on distribution which matches the target. The energy score will be super low, and high likelihood for samples. However, we're completely ignoring the rest of the network and the classification labels. Different PTMs with similar embedding layers might be clustered together, regardless of their accuracy. Thus, use classification based scores like LogME, LEEP, PACTran and $$S_{cls}$$ in addition to the $$S_{en}$$ score. Reference the following table. 

{% include figure.liquid loading="eager" path="assets/img/tldr/etran/table.jpeg" title="example image" class="img-fluid rounded z-depth-1" %}


## Future Work? 
